# Semantic Scene Mapping with Spatio-temporal Deep Neural Network for Robotic Applications
# https://link.springer.com/article/10.1007/s12559-017-9526-9
@Article{Li2018,
author="Li, Ruihao
and Gu, Dongbing
and Liu, Qiang
and Long, Zhiqiang
and Hu, Huosheng",
title="Semantic Scene Mapping with Spatio-temporal Deep Neural Network for Robotic Applications",
journal="Cognitive Computation",
year="2018",
month="Apr",
day="01",
volume="10",
number="2",
pages="260--271",
abstract="Semantic scene mapping is a challenge and significant task for robotic application, such as autonomous navigation and robot-environment interaction. In this paper, we propose a semantic pixel-wise mapping system for potential robotic applications. The system includes a novel spatio-temporal deep neural network for semantic segmentation and a Simultaneous Localisation and Mapping (SLAM) algorithm for 3D point cloud map. Their combination yields a 3D semantic pixel-wise map. The proposed network consists of Convolutional Neural Networks (CNNs) with two streams: spatial stream with images as the input and temporal stream with image differences as the input. Due to the use of both spatial and temporal information, it is called spatio-temporal deep neural network, which shows a better performance in both accuracy and robustness in semantic segmentation. Further, only keyframes are selected for semantic segmentation in order to reduce the computational burden for video streams and improve the real-time performance. Based on the result of semantic segmentation, a 3D semantic map is built up by using the 3D point cloud map from a SLAM algorithm. The proposed spatio-temporal neural network is evaluated on both Cityscapes benchmark (a public dataset) and Essex Indoor benchmark (a dataset we labelled ourselves manually). Compared with the state-of-the-art spatial only neural networks, the proposed network achieves better performances in both pixel-wise accuracy and Intersection over Union (IoU) for scene segmentation. The constructed 3D semantic map with our methods is accurate and meaningful for robotic applications.",
issn="1866-9964",
doi="10.1007/s12559-017-9526-9",
url="https://doi.org/10.1007/s12559-017-9526-9"
}

